{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM architecture.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSCOYN6-kiWE"
      },
      "source": [
        "#### Importing the classes and functions required for this & initializing the random number generator to a constant value to ensure we can easily reproduce the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68W2TnHdkQfv"
      },
      "source": [
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM \n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "numpy.random.seed(7)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk7uNp9SkQiT",
        "outputId": "5eece800-abec-43e3-de47-6f06ea50d77f"
      },
      "source": [
        "# we are going to take top 4000 words of the imdb review\n",
        "top_words = 4000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0E0h28Lksh7"
      },
      "source": [
        "#### there is a high chance that we may have unequal length of the sentence- so MAKE IT EQUAL SIZED VECTOR by padding and truncation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3xfszq8kQlA"
      },
      "source": [
        "max_review_length = 400\n",
        "X_train= sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test= sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTaPrw5ZkQnk",
        "outputId": "8f78368a-d196-4d0a-80fe-4253cf913ef2"
      },
      "source": [
        "print(X_test[25])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    1  146   24\n",
            "  252  138   14   22    9    2   38  364   23  899   54   45  427  285\n",
            "   25  100  126  181   11    6  189   22   14    9    4    2    7  112\n",
            "    6  189   22   13 1133  546    8   30    6  194  189  337    5   13\n",
            "  215  135   15   14  313 1545    4    2  313    7    2    9    4   65\n",
            "    7    6  604    7 1170  362   23   68   96    8    6 2319  500  937\n",
            " 1197    8 1250   46   18    4  314    5   28    6  521   11   19    6\n",
            "  719    2  725    2    4  375 1972   36   97    6 2943 3808    5 1197\n",
            "    8  140   83  513   18    6 1912  519  173    4  513    9  948    5\n",
            "  146   43 2146  570   50   88 1584   54    4  371 2943    2  778 1684\n",
            "   72   54   13  135   48  129  267   18    6  189   22  140   67   14\n",
            "   25   80  119   12   45 1669    2    5    2   19   45  880  139    4\n",
            "   65    9  221    5    4  105   26    2 1309   19    4  156  743   98\n",
            "    2    7 1613 1450    2 1926  937  127  179   73   19   41  173    4\n",
            "   22 1838    6  117   23    4 2012  499   21   45   38   76  253    5\n",
            "   37 2263   14   20  144 1900   56    4  953 1052    5   32  189  451\n",
            "  144   28  173   11   12  140   67  313    7    2    4   22   15  944\n",
            " 2391  112    2 1165    2    2  351 1626    2  893    2    2   23    6\n",
            "  131  581  415    6    2  112  605  125    6    2    6    2  143    4\n",
            "  419    5   76   53   13   16 2042  195    8 2413   14   22   33    4\n",
            "    2    2    5   32    4  156   71   23  508    8    2    4   22    5\n",
            "  430   81   36   28  142    8   30 2606    7  313    7    2    9    4\n",
            "    2    2 1365    7    4  291  790  158]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8lafyk-k7fX"
      },
      "source": [
        "#### Now we will define, compile and fit our LSTM model\n",
        "---------\n",
        "#### embedding layer is the first layer that uses vector of size 30 to represent each words\n",
        "#### ---------\n",
        "#### Next is LSTM layer which is containing 100 NEURONS.\n",
        "#### This is a classification problem so lastly we will use dense output layer with a single neuron and a sigmoid activation function to make 0 & 1 as the prediction for good or bad review.\n",
        "#### its a binary classification so we will use log loss as our COST FUNCTION (binary_crossentropy in Keras).\n",
        "#### till here we defined the requirements/parameters of our model now we need to reach global minima for the cost function equation. we can use any optimization technique for our convergence such as Adagrad, RMSProp etc (here we will ADAM optimizer).\n",
        "#### after deciding the optimization technique we dont want to feed whole data at once so we will use a BATCH SIZE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2fChMVXkQp4",
        "outputId": "bc665682-c282-47ff-e93c-b4b61582919d"
      },
      "source": [
        "embedding_vector_length = 30\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words,embedding_vector_length, input_length = max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compilation\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "# fitting model\n",
        "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=2, batch_size=60)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 400, 30)           120000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               52400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 172,501\n",
            "Trainable params: 172,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            "417/417 [==============================] - 46s 32ms/step - loss: 0.5677 - accuracy: 0.6869 - val_loss: 0.3499 - val_accuracy: 0.8551\n",
            "Epoch 2/2\n",
            "417/417 [==============================] - 13s 30ms/step - loss: 0.3040 - accuracy: 0.8779 - val_loss: 0.3353 - val_accuracy: 0.8578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3d30dd5690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4luJ9ugRkQsZ",
        "outputId": "212c3be4-bb83-403f-c992-02e6766a0ac6"
      },
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 85.78%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttvYUZJmkQvS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZZeqjN6kQyO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuL04_I6kQ1j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX9Te7LdkQ4e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvBcVM2ZkQ7S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCgljik4kQ9t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}